{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wragle_report\n",
    "* Create a **300-600 word written report** called \"wrangle_report.pdf\" or \"wrangle_report.html\" that briefly describes your wrangling efforts. This is to be framed as an internal document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The objectives of the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim and objective of this project is to wrangle the data of dog ratings, the data source is from the twitter user @WeRateDogs. Gathering, assessing and cleaning would be done to prepare the data for analysis.\n",
    "The process would be discussed as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Three data were provided in this project which are Twitter archive file, Tweet image prediction file and the Tweet_Json text.\n",
    "\n",
    "The first data which is the Twitter archive file was already provided bu Udacity, i downloaded it into my project workspace by clicking on the yellow jupyter icon at the top right corner and then uploaded the file. I my work space i imported the pandas library that i used to read the file into a dataframe using a pandas method called read_csv() and stored the result using a variable called df_A.\n",
    "    Secondly, downloaded the second data image prediction file file programmatically using the request and os libraries that i mported at the begining and used the with open() method of the request library to generate a response of 200 which i obtained success. Then i wrote the response content into a tsv file and named it image-prediction.tsv which i later read it into a dataframe then called it image_prediction\n",
    "    Lastly, creating a twitter developer account and creating an application for the project. I used the app credentials (consumer_key, consumer_secret, access_toke, and access_secret), for the twitter API i imported tweepy and json, authenticated tweepy.OAuthHandler and set wait_on_limit to True. I used the code already provided by udacity to extract the tweet id and created the file tweet_json.txt .\n",
    "with the python with open() function i read the tweet_json.txt line by line and loaded each line as json file, i then read it into a pandas dataframe saving the result using a variable called tweetcount\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used two method to assess the data;\n",
    "\n",
    "Visual assessment: i printed out all the data by calling their variable names used to store them to see hoh they look like\n",
    "\n",
    "Programmmatic assessment: i used the pandas function such as .info(), .describe(), .isnull(), .head() and .value_counts() method to assess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    At this point made a copy of all the data and created an new variable name by adding _clean to their previous variable names that is df_A as df_A_clean, image_prediction as image_prediction_clean and tweetcount as tweetcount_clean.\n",
    "    i then define the problem each data has, code the problem in order to solve it and test to see the outcome, the following were done;\n",
    "1. replaced the 'None', 'a', 'an' and 'the' etc. values with NaN using the replace method\n",
    "\n",
    "2. replaced the 'None' values with NAN using the replace method and dropping the NaN values\n",
    "\n",
    "3. masked the df_A_clean to remove the retweeted data using the isnull() method\n",
    "\n",
    "4. used the pandas drop method to drop the columns 'in_reply_to_status_id', 'in_reply_to_user_id', 'retweeted_status_id',         'retweeted_status_user_id', 'retweeted_status_timestamp all have more than 50% of its data missing\n",
    "\n",
    "5. changed the datatype of the 'timestamp' column using the pandas to_datatime funtion\n",
    "\n",
    "6. used the regular expression method to extract the name of the source\n",
    "\n",
    "7. used the pandas rename method to rename the colums  'img_num' and 'jpg_url'\n",
    "\n",
    "8. used the pandas rename method to rename the 'id' column\n",
    "\n",
    "9. used the pandas DatetimeIndex function to split the 'timestamp' into 'year', 'month' and 'day'\n",
    "\n",
    "10. used the pandas string(split) and indexing method to remove dulipcate links\n",
    "\n",
    "11. i merged the three dataframes together "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## storing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the gathering assessing and cleaning processes were concluded i  saved the merged data in a csv file named twitter_archive_master.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data wrangling process was quite hectic and tidious but i was able to apply the methods that i learnt from the course which was exciting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
